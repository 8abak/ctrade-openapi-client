<!-- docs/architecture.md -->

# System Architecture — Segmeling / datavis.au

This document defines the complete architecture used by the segmeling (datavis.au) project.  
All code, design, and modifications must follow this structure.  
Custom GPTs must read this file before proposing or writing any code.

---

## 1. System Purpose

Segmeling is a multi-layer analytical and machine learning system for studying **XAUUSD tick data**, building hierarchical understanding (ticks → segments → zones → pivots → ML predictions), and visualizing it live and historically at **https://datavis.au**.

The project includes:

1. Tick ingestion  
2. PostgreSQL storage  
3. Backend API  
4. Frontend chart system  
5. Jobs / ML processing  
6. DevOps pipeline (GitHub → EC2)

All components must remain consistent with the definitions in this file.

---

## 2. Project Structure

The expected project structure is:

    project-root/
      backend/
        main.py          # All API routes (FastAPI or equivalent)
        db.py            # Pure DB helper functions
      docs/
        architecture.md
        workflow.md
        decisions.md
        schema.md
        db-schema.txt    # Auto-generated latest schema snapshot
        routes-and-db.txt
      jobs/
        buildSchema.py   # Writes docs/db-schema.txt
        buildRoots.py    # Writes docs/routes-and-db.txt
        ...              # Other offline tools and ML jobs
      ml/
        ...              # Machine learning scripts
      tickCollector/
        ...              # Live tick ingestion from cTrader API
      frontend/
        chart-core.js    # Shared ECharts engine (global chart behavior)
        tick-core.js     # Live view controller
        htick-core.js    # Historical view controller
        review-core.js   # Prediction review controller
        *.html           # Pages
      (CI/CD and deploy scripts)

All modifications must preserve this structure unless a change is recorded in `docs/decisions.md`.

---

## 3. Database Architecture

Database: PostgreSQL, schema: `public`.

### 3.1 Global Rules

- Every table must have a primary key column named `id` as the **first** column.  
- Column names must be concise and consistent.  
- Any schema change requires:
  - An entry in `docs/decisions.md`.  
  - Regenerating `docs/db-schema.txt` using `python -m jobs.buildSchema`.

### 3.2 Core Tables (non-exhaustive list)

- `ticks` – raw tick stream (id, timestamp, bid, ask, mid, kal, and related fields).  
- `zones`, `zpers` – segment and zone personality tables.  
- `piv`, `swg` – pivot and swing tables.  
- `atr*`, `zig*` – ATR-based and zigzag-based layers.  
- `labels_*` – manual and ML-generated label tables.

The **canonical** and up-to-date schema (all tables, columns, indexes, constraints) is always in:

- `docs/db-schema.txt`

Custom GPTs must read `docs/db-schema.txt` before proposing any SQL or schema changes.

---

## 4. Backend Architecture

### 4.1 `backend/main.py`

Responsibilities:

- Define **all API routes** exposed to the frontend or jobs.
- Handle:
  - Request validation.
  - Calling helper functions in `backend/db.py` or other modules.
  - Returning JSON responses.

Restrictions:

- No heavy business logic; long-running or complex processing goes into `jobs/` or `ml/`.
- No direct SQL in `main.py` – SQL goes into `db.py` or job modules.

Route naming convention (conceptual, not enforced by code):

- `/api/<entity>/<action>`

Examples:

- `/api/ticks/range`  
- `/api/ticks/latest`  
- `/api/zones/range`  
- `/api/labels/create`

Actual existing routes are listed in `docs/routes-and-db.txt`, generated by `jobs/buildRoots.py`.

### 4.2 `backend/db.py`

Responsibilities:

- Provide pure database helper functions for use by `main.py` and jobs.

Rules:

- No FastAPI imports.  
- No HTTP logic.  
- No chart or visualization logic.  
- No ML algorithms.  

Functions typically look like:

- `GetTicksRange(StartId, EndId)`  
- `GetLatestTicks(Limit)`  
- `InsertZone(...)`  

The list of detected DB helper functions is also in `docs/routes-and-db.txt`.

---

## 5. Frontend Architecture

### 5.1 `frontend/chart-core.js` (shared chart engine)

All ECharts-based chart behavior must be implemented in `chart-core.js`.  
This file is the **only** place where chart behavior is defined.

Responsibilities:

- Creating and configuring the ECharts instance.  
- Rendering ticks (bid, ask, mid, kal, etc.).  
- Rendering overlays (zones, pivots, predictions, labels, other layers).  
- Managing pan and zoom behavior for time and price axes.  
- Defining tooltip content (tick id, time, prices, extra fields).  

Any change to chart behavior (how things look or move) must be made here, not in page-specific files.

### 5.2 Page Controllers

The following files act as page-specific controllers and must call functions from `chart-core.js`:

- `tick-core.js` – live tick view.  
- `htick-core.js` – historical tick view and label tools.  
- `review-core.js` – playback view with predictions.

They should interact with the chart engine via a stable public API (conceptual example):

- `InitChart(DomElement, Options)`  
- `UpdateTicks(ChartHandle, TickList)`  
- `UpdateOverlays(ChartHandle, OverlayConfig)`  
- `SetRunState(ChartHandle, IsRunning)`  
- `SetVisibleLayers(ChartHandle, LayerConfig)`

Page controllers:

- May handle user interface logic specific to that page (buttons, sliders, checkboxes).  
- Must not duplicate chart behavior already handled by `chart-core.js`.

---

## 6. Jobs and ML Architecture

All long-running tasks, data preparation, and machine learning logic must live in:

- `jobs/`  
- `ml/`

Responsibilities:

- Batch processing of historical data.  
- Building features and labels.  
- Training and evaluating models.  
- Running recurring maintenance tasks.

Rules:

- Jobs must not silently alter DB schema.  
- Jobs must not modify backend or frontend code.  
- Jobs must be executable using:

      python -m jobs.<JobName>

---

## 7. Deployment and DevOps

Source of truth: **GitHub repository**.

Deployment target: **EC2 instance**.

Typical flow:

1. Developer commits and pushes to GitHub.  
2. GitHub Actions (or a similar workflow) connects to EC2, pulls latest code, and restarts services.  
3. On EC2, after pulling code and activating the virtual environment:

       python -m jobs.buildSchema
       python -m jobs.buildRoots

   This regenerates:

   - `docs/db-schema.txt`
   - `docs/routes-and-db.txt`

4. Backend services (e.g., `backend.service`) are restarted via `systemctl`.

Custom GPTs must respect this deployment pattern when suggesting DevOps changes or job automation.

---
